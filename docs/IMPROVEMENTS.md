# OpenTelemetry Demo - Improvements Documentation

## Overview

This document outlines all improvements made to the OpenTelemetry demo application to enhance observability, particularly focusing on business metrics and APM (Application Performance Monitoring) capabilities.

## Table of Contents

- [Executive Summary](#executive-summary)
- [Improvements Implemented](#improvements-implemented)
- [Architecture Changes](#architecture-changes)
- [Metrics Added](#metrics-added)
- [Testing](#testing)
- [Dashboard Usage](#dashboard-usage)
- [Troubleshooting](#troubleshooting)

---

## Executive Summary

### Problem Statement

The Executive Dashboard - Business Metrics was not displaying data correctly due to:
1. **Missing HTTP latency instrumentation** - No histogram for measuring request duration
2. **Inconsistent metric nomenclature** - Python used `http.requests.total` vs expected `http_requests_total`
3. **Missing status code labels** - HTTP status codes weren't being tracked as labels
4. **Incomplete APM instrumentation** - Lack of automatic RED metrics capture

### Solution

Implemented comprehensive APM instrumentation in both Node.js and Python applications with:
- ‚úÖ Automatic latency tracking via middleware
- ‚úÖ HTTP status code labeling
- ‚úÖ Consistent metric naming across services
- ‚úÖ Enhanced business metrics for executive decision-making
- ‚úÖ Complete trace-log-metric correlation

---

## Improvements Implemented

### 1. Node.js Application (`src/otel-app/index.js`)

#### 1.1 RED Metrics Infrastructure

**Added automatic HTTP instrumentation middleware:**

```javascript
// APM MIDDLEWARE - Automatic instrumentation for all endpoints
app.use((req, res, next) => {
  const startTime = Date.now();

  // Intercept response to capture metrics
  const captureMetrics = (body) => {
    const duration = Date.now() - startTime;
    const endpoint = req.route?.path || req.path || 'unknown';
    const method = req.method;
    const statusCode = res.statusCode;

    // Record HTTP request counter with status code
    requestCounter.add(1, {
      endpoint,
      method,
      http_status_code: statusCode.toString(),
      service_name: 'products-service',
    });

    // Record HTTP server duration (latency) for SLO monitoring
    httpServerDuration.record(duration, {
      endpoint,
      method,
      http_status_code: statusCode.toString(),
      service_name: 'products-service',
    });
  };

  // ... override res.json and res.send
});
```

**Benefits:**
- **Zero-touch instrumentation** - All endpoints automatically tracked
- **Consistent labeling** - Same labels across all metrics
- **SLO monitoring** - Latency percentiles (p50, p95, p99) available
- **Error tracking** - Automatic categorization by HTTP status code

#### 1.2 HTTP Server Duration Histogram

**Added new metric:**

```javascript
const httpServerDuration = meter.createHistogram('http_server_duration', {
  description: 'HTTP server request duration in milliseconds',
  unit: 'ms',
});
```

**Prometheus queries enabled:**
```promql
# p95 latency
histogram_quantile(0.95, sum(rate(http_server_duration_bucket[5m])) by (le, service_name))

# p99 latency
histogram_quantile(0.99, sum(rate(http_server_duration_bucket[5m])) by (le, service_name))
```

#### 1.3 Refactored Metric Collection

**Before:**
- Manual `requestCounter.add()` in every endpoint
- No latency tracking
- Missing status code labels

**After:**
- Automatic tracking via middleware
- Removed 7 redundant manual metric calls
- Consistent labeling across all endpoints

---

### 2. Python Application (`src/otel-python-app/app.py`)

#### 2.1 Metric Nomenclature Standardization

**Changed metric names for Prometheus compatibility:**

**Before:**
```python
request_counter = meter.create_counter('http.requests.total', ...)
order_counter = meter.create_counter('orders.created.total', ...)
order_value_histogram = meter.create_histogram('orders.value', ...)
```

**After:**
```python
request_counter = meter.create_counter('http_requests_total', ...)
order_counter = meter.create_counter('orders_created_total', ...)
order_value_histogram = meter.create_histogram('orders_value', ...)
```

**Rationale:**
- Prometheus convention uses underscores, not dots
- Ensures dashboard queries work correctly
- Aligns with Node.js naming convention

#### 2.2 Flask APM Middleware

**Added automatic instrumentation using Flask hooks:**

```python
@app.before_request
def before_request():
    """Store request start time for latency calculation"""
    from flask import g
    g.start_time = time.time()

@app.after_request
def after_request(response):
    """Capture metrics after request completes"""
    if hasattr(g, 'start_time'):
        duration_ms = (time.time() - g.start_time) * 1000

        # Record HTTP request counter with status code
        request_counter.add(1, {
            'endpoint': endpoint,
            'method': method,
            'http_status_code': str(status_code),
            'service_name': 'orders-service',
        })

        # Record HTTP server duration
        http_server_duration.record(duration_ms, {
            'endpoint': endpoint,
            'method': method,
            'http_status_code': str(status_code),
            'service_name': 'orders-service',
        })

    return response
```

**Benefits:**
- Consistent with Node.js middleware approach
- Automatic metric collection for all endpoints
- Removed 6 redundant manual metric calls

#### 2.3 HTTP Server Duration Histogram

**Added new metric:**

```python
http_server_duration = meter.create_histogram(
    'http_server_duration',
    description='HTTP server request duration in milliseconds',
    unit='ms',
)
```

---

### 3. Traffic Generation Script (`generate-traffic.sh`)

**Created comprehensive traffic simulator with realistic e-commerce scenarios:**

#### Scenarios Implemented:

1. **User Browsing (60% of traffic)**
   - Browse all products
   - Filter by category
   - View product details
   - Check inventory

2. **Successful Orders (30% of traffic)**
   - Complete purchase flow
   - Distributed tracing across services
   - Revenue metrics captured

3. **Failed Orders - Insufficient Inventory (5% of traffic)**
   - Triggers `cart_abandonment_total` metric
   - Generates `revenue_at_risk_dollars` metric
   - Creates error traces

4. **Failed Orders - Invalid Product (2% of traffic)**
   - 404 errors tracked
   - Error budget consumption

5. **Service Errors (1% of traffic)**
   - Intentional 500 errors
   - Tests error handling and alerting

6. **Cart Abandonment (15% of traffic)**
   - Simulates user behavior
   - Tracks conversion funnel

7. **Health Checks (every 10 requests)**
   - Service availability monitoring
   - Uptime tracking

#### Usage:

```bash
# Default: 100 iterations with 0.5s delay
./generate-traffic.sh

# Custom configuration
ITERATIONS=500 DELAY=0.2 ./generate-traffic.sh

# Custom hosts
PRODUCTS_HOST="my-products.example.com" ORDERS_HOST="my-orders.example.com" ./generate-traffic.sh
```

#### Output Statistics:

```
üìà Statistics:
   ‚úì Successful Orders:  30
   ‚úó Failed Orders:      7
   üëÅ Products Viewed:   63
   ‚ö† Errors Triggered:  1
```

---

## Architecture Changes

### Observability Stack

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Application Layer                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ  Products Service‚îÇ         ‚îÇ   Orders Service  ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ    (Node.js)     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§     (Python)     ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ           ‚îÇ                             ‚îÇ                    ‚îÇ
‚îÇ           ‚îÇ  OTLP HTTP (port 4318)      ‚îÇ                    ‚îÇ
‚îÇ           ‚îÇ  - Traces                   ‚îÇ                    ‚îÇ
‚îÇ           ‚îÇ  - Metrics                  ‚îÇ                    ‚îÇ
‚îÇ           ‚îÇ  - Logs                     ‚îÇ                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                             ‚îÇ
            ‚ñº                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ               OpenTelemetry Collector                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Processors:                                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - batch                                           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    - k8sattributes (enrichment with pod metadata)   ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ Traces       ‚îÇ  Metrics     ‚îÇ  Logs               ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ Exporter     ‚îÇ  Exporter    ‚îÇ  Exporter           ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ (OTLP)       ‚îÇ  (Prometheus)‚îÇ  (OTLP/HTTP)        ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ              ‚îÇ                 ‚îÇ
          ‚ñº              ‚ñº                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Tempo    ‚îÇ  ‚îÇ  Prometheus  ‚îÇ  ‚îÇ    Loki     ‚îÇ
‚îÇ  (Traces)   ‚îÇ  ‚îÇ  (Metrics)   ‚îÇ  ‚îÇ   (Logs)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                ‚îÇ                  ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚ñº
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ   Grafana    ‚îÇ
                ‚îÇ (Dashboard)  ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Metric Flow

```
Application ‚Üí OpenTelemetry SDK ‚Üí OTLP Exporter ‚Üí
OTel Collector ‚Üí Prometheus Exporter ‚Üí Prometheus ‚Üí Grafana
```

**Key transformation:**
- OTel histogram ‚Üí Prometheus histogram with `_sum`, `_count`, `_bucket` suffixes
- Resource attributes ‚Üí Prometheus labels (via `resource_to_telemetry_conversion: true`)

---

## Metrics Added

### RED Metrics (Rate, Errors, Duration)

| Metric Name | Type | Labels | Description |
|------------|------|--------|-------------|
| `http_requests_total` | Counter | `endpoint`, `method`, `http_status_code`, `service_name` | Total HTTP requests by endpoint and status |
| `http_server_duration` | Histogram | `endpoint`, `method`, `http_status_code`, `service_name` | Request latency in milliseconds |

**Dashboard queries:**

```promql
# Request rate
sum(rate(http_requests_total[5m])) by (service_name)

# Error rate
sum(rate(http_requests_total{http_status_code=~"5.."}[5m])) by (service_name)
/ sum(rate(http_requests_total[5m])) by (service_name)

# p95 latency
histogram_quantile(0.95, sum(rate(http_server_duration_bucket[5m])) by (le, service_name))
```

### Business Metrics

#### Products Service (Node.js)

| Metric Name | Type | Purpose |
|------------|------|---------|
| `revenue_at_risk_dollars` | Counter | Tracks potential revenue loss from failures |
| `cart_abandonment_total` | Counter | Counts abandoned carts (by reason) |
| `checkout_attempts_total` | Counter | Total checkout attempts |
| `checkout_success_total` | Counter | Successful checkouts |
| `customer_experience_score` | ObservableGauge | CX score (0-100) based on latency |
| `transaction_value_dollars` | Histogram | Distribution of transaction values |

#### Orders Service (Python)

| Metric Name | Type | Purpose |
|------------|------|---------|
| `order_revenue_dollars` | Histogram | Distribution of order revenue |
| `failed_transaction_revenue_lost` | Counter | Revenue lost from failed transactions |
| `order_processing_time_seconds` | Histogram | Order processing duration (SLA metric) |
| `sla_violation_events` | Counter | Count of SLA violations (>2s threshold) |

---

## Testing

### Prerequisites

```bash
# Ensure demo is running
kubectl get pods -n monitoring
kubectl get pods -n default

# Verify services are accessible
curl -H "Host: products.127.0.0.1.nip.io" http://products.127.0.0.1.nip.io/health
curl -H "Host: orders.127.0.0.1.nip.io" http://orders.127.0.0.1.nip.io/health
```

### Generate Test Traffic

```bash
# Generate 100 requests (default)
./generate-traffic.sh

# Generate continuous traffic for 5 minutes
ITERATIONS=600 DELAY=0.5 ./generate-traffic.sh

# High-volume load test
ITERATIONS=1000 DELAY=0.1 ./generate-traffic.sh
```

### Verify Metrics in Prometheus

```bash
# Port-forward Prometheus
kubectl port-forward -n monitoring svc/prometheus-server 9090:80

# Open browser
http://localhost:9090

# Query metrics
http_requests_total
http_server_duration_bucket
revenue_at_risk_dollars_total
checkout_success_total
```

### Verify Traces in Grafana

```bash
# Access Grafana
http://grafana.127.0.0.1.nip.io

# Navigate to: Explore ‚Üí Tempo ‚Üí Query
# Example TraceQL queries:
{ resource.service.name = "orders-service" && status = error }
{ duration > 500ms }
```

---

## Dashboard Usage

### Executive Dashboard - Business Metrics

**Location:** Grafana ‚Üí Dashboards ‚Üí Executive Dashboard - Business Metrics

#### Key Panels:

**1. Business Impact Summary**

- **Revenue at Risk ($/min):** Real-time revenue loss from failures
  - Green: <$50/min
  - Yellow: $50-100/min
  - Orange: $100-500/min
  - Red: >$500/min

- **Affected Customers:** Count of customers impacted by cart abandonment

- **Avg Order Processing Time:** MTTR proxy metric
  - Green: <1s
  - Yellow: 1-2s
  - Orange: 2-3s
  - Red: >3s

- **Customer Experience Score:** 0-100 based on latency
  - Excellent: 85-100 (latency <150ms)
  - Good: 70-85 (latency 150-300ms)
  - Poor: <70 (latency >300ms)

**2. SLA/SLO Compliance**

- **Availability SLA:** Target 99.9%
  - Measures: `(1 - 5xx errors / total requests) * 100`

- **Latency SLO p95:** Target <200ms
  - Uses `order_processing_time_seconds` histogram

- **Error Budget:** Remaining budget for current hour
  - Formula: `100 - (SLA violations / checkout attempts) * 100`

**3. Customer Experience Metrics**

- **Checkout Success Rate:** `(checkout_success / checkout_attempts) * 100`
  - Red: <70%
  - Yellow: 70-85%
  - Green: >95%

- **Cart Abandonment Rate:** Percentage of abandoned carts

**4. Operational Efficiency**

- **Incidents Detected (Last 7 Days):** Total SLA violations
- **MTTR:** Mean time to resolution
- **Cost Savings:** Estimated revenue saved through early detection

#### Interpreting the Dashboard

**Green panels:** Services are healthy, meeting SLAs
**Yellow panels:** Warning - approaching SLA limits
**Red panels:** Critical - immediate action required

**Example alert scenarios:**

1. **Revenue at Risk >$500/min:** Indicates major service degradation
   - **Action:** Check service logs and traces for root cause
   - **Grafana:** Navigate to Distributed Tracing Dashboard

2. **Checkout Success Rate <70%:** Payment gateway or inventory issues
   - **Action:** Check `checkout_attempts_total` vs `checkout_success_total` by reason
   - **Query:** `sum by (reason) (rate(cart_abandonment_total[5m]))`

3. **Availability SLA <99%:** High error rate
   - **Action:** Check error distribution by service
   - **Query:** `topk(3, sum by (service_name) (rate(http_requests_total{http_status_code=~"5.."}[5m])))`

---

## Troubleshooting

### Dashboard Shows "No Data"

**Symptoms:**
- Executive Dashboard panels are empty
- Queries return no results in Prometheus

**Diagnosis:**

1. **Check if traffic is being generated:**
   ```bash
   ./generate-traffic.sh
   ```

2. **Verify OTel Collector is receiving metrics:**
   ```bash
   kubectl logs -n monitoring deploy/opentelemetry-collector | grep "Exporting metrics"
   ```

3. **Check Prometheus is scraping Collector:**
   ```bash
   kubectl port-forward -n monitoring svc/prometheus-server 9090:80
   # Open http://localhost:9090/targets
   # Look for "opentelemetry-collector" target (should be UP)
   ```

4. **Verify metric names in Prometheus:**
   ```promql
   # List all metrics with "http" prefix
   {__name__=~"http.*"}

   # Check specific metric
   http_requests_total
   ```

**Resolution:**

If metrics are not appearing:
```bash
# Restart OTel Collector
kubectl rollout restart -n monitoring deployment/opentelemetry-collector

# Restart applications
kubectl rollout restart deployment/otel-demo-app
kubectl rollout restart deployment/otel-python-app

# Wait for pods to be ready
kubectl wait --for=condition=ready pod -l app=otel-demo-app --timeout=60s
kubectl wait --for=condition=ready pod -l app=otel-python-app --timeout=60s

# Generate traffic
./generate-traffic.sh
```

### Histogram Queries Return NaN

**Symptoms:**
- `histogram_quantile()` returns NaN
- p95, p99 latency panels are empty

**Diagnosis:**

Check if histogram buckets exist:
```promql
http_server_duration_bucket
```

If empty, the metric is not being recorded.

**Resolution:**

1. **Verify histogram is created:**
   ```javascript
   // Node.js
   const httpServerDuration = meter.createHistogram('http_server_duration', {
     description: 'HTTP server request duration in milliseconds',
     unit: 'ms',
   });
   ```

2. **Verify histogram is recorded:**
   ```javascript
   httpServerDuration.record(duration, { /* labels */ });
   ```

3. **Check label cardinality:**
   ```promql
   # Count unique label combinations
   count(http_server_duration_bucket) by (endpoint, method, http_status_code)
   ```

   If cardinality is too high (>1000), reduce labels.

### Metrics Have Wrong Names

**Symptoms:**
- Python metrics show as `http.requests.total` instead of `http_requests_total`
- Dashboard queries fail to find metrics

**Resolution:**

Verify metric naming convention:
```python
# WRONG
request_counter = meter.create_counter('http.requests.total', ...)

# CORRECT
request_counter = meter.create_counter('http_requests_total', ...)
```

Prometheus converts dots to underscores, but it's best practice to use underscores directly.

### SLA Violation Metrics Missing

**Symptoms:**
- `sla_violation_events_total` returns no data
- Error Budget panel shows 100%

**Diagnosis:**

Check if SLA threshold is being exceeded:
```python
# Python app.py
if processing_time > 2.0:  # 2 second SLA threshold
    sla_violation_counter.add(1, {
        'service_name': 'orders-service',
        'reason': 'slow_processing'
    })
```

**Resolution:**

Generate slow requests to trigger SLA violations:
```bash
# Create high inventory quantity to slow down processing
curl -X POST \
  -H "Host: orders.127.0.0.1.nip.io" \
  -H "Content-Type: application/json" \
  -d '{"product_id": 1, "quantity": 1000, "user_id": "test-user"}' \
  http://orders.127.0.0.1.nip.io/api/orders
```

---

## Best Practices for Production

### 1. Label Cardinality Management

**Problem:** High cardinality labels (e.g., `user_id`, `order_id`) can cause Prometheus performance issues.

**Solution:**
- ‚úÖ Use: `endpoint`, `method`, `http_status_code`, `service_name`
- ‚ùå Avoid: `user_id`, `order_id`, `transaction_id`, `ip_address`

**Cardinality limits:**
- Keep unique label combinations < 10,000 per metric
- Monitor Prometheus cardinality: `/metrics` endpoint

### 2. Histogram Bucket Configuration

**Default buckets may not fit your SLO:**

```javascript
// Customize histogram buckets for your latency requirements
const httpServerDuration = meter.createHistogram('http_server_duration', {
  description: 'HTTP server request duration in milliseconds',
  unit: 'ms',
  boundaries: [10, 50, 100, 200, 500, 1000, 2000, 5000], // Custom buckets
});
```

### 3. Sampling for High-Volume Services

**For services with >1000 req/s, use sampling:**

```javascript
// Tail-based sampling in OTel Collector
processors:
  tail_sampling:
    decision_wait: 10s
    policies:
      - name: error-traces
        type: status_code
        status_code: {status_codes: [ERROR]}
      - name: slow-traces
        type: latency
        latency: {threshold_ms: 500}
      - name: random-sample
        type: probabilistic
        probabilistic: {sampling_percentage: 10}
```

### 4. Alerting Rules

**Create Prometheus alerting rules for critical metrics:**

```yaml
groups:
  - name: business_metrics
    interval: 30s
    rules:
      - alert: HighRevenueAtRisk
        expr: sum(rate(revenue_at_risk_dollars_total[5m])) * 60 > 500
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Revenue at risk exceeds $500/min"
          description: "Current rate: {{ $value | printf \"%.2f\" }} $/min"

      - alert: LowCheckoutSuccessRate
        expr: (sum(rate(checkout_success_total[5m])) / sum(rate(checkout_attempts_total[5m]))) * 100 < 70
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Checkout success rate below 70%"

      - alert: SLAViolation
        expr: sum(rate(sla_violation_events_total[1h])) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "More than 10 SLA violations in the last hour"
```

### 5. Cost Optimization

**Reduce metrics storage costs:**

1. **Metric retention:**
   ```yaml
   # prometheus.yaml
   storage:
     tsdb:
       retention.time: 15d  # Adjust based on compliance requirements
   ```

2. **Recording rules for expensive queries:**
   ```yaml
   groups:
     - name: precomputed_metrics
       interval: 30s
       rules:
         - record: job:http_requests:rate5m
           expr: sum(rate(http_requests_total[5m])) by (service_name)
   ```

3. **Metric relabeling to drop high-cardinality labels:**
   ```yaml
   metric_relabel_configs:
     - source_labels: [user_id]
       regex: '.*'
       action: labeldrop
   ```

---

## Metrics Reference

### Complete Metrics List

#### Infrastructure Metrics (Both Services)

| Metric | Type | Labels | Unit | Description |
|--------|------|--------|------|-------------|
| `http_requests_total` | Counter | `endpoint`, `method`, `http_status_code`, `service_name` | count | Total HTTP requests |
| `http_server_duration` | Histogram | `endpoint`, `method`, `http_status_code`, `service_name` | ms | Request latency |

#### Business Metrics - Products Service

| Metric | Type | Labels | Unit | Description |
|--------|------|--------|------|-------------|
| `products_viewed_total` | Counter | `product_id`, `product_name`, `category`, `service_name` | count | Product views |
| `purchases_total` | Counter | `product_id`, `product_name`, `service_name` | count | Completed purchases |
| `inventory_level` | ObservableGauge | `product_id`, `product_name`, `category` | units | Current stock |
| `revenue_at_risk_dollars` | Counter | `service_name`, `reason`, `product_name` | USD | Revenue at risk |
| `transaction_value_dollars` | Histogram | `service_name`, `product_id`, `product_name` | USD | Transaction values |
| `cart_abandonment_total` | Counter | `service_name`, `reason`, `product_id` | count | Cart abandonments |
| `checkout_attempts_total` | Counter | `service_name`, `product_id` | count | Checkout attempts |
| `checkout_success_total` | Counter | `service_name`, `product_id` | count | Successful checkouts |
| `customer_experience_score` | ObservableGauge | `service_name` | 0-100 | CX score |

#### Business Metrics - Orders Service

| Metric | Type | Labels | Unit | Description |
|--------|------|--------|------|-------------|
| `orders_created_total` | Counter | `product_id`, `user_id`, `service_name` | count | Orders created |
| `orders_value` | Histogram | `product_id`, `service_name` | USD | Order values |
| `order_revenue_dollars` | Histogram | `service_name`, `product_id`, `user_id` | USD | Order revenue |
| `failed_transaction_revenue_lost` | Counter | `service_name`, `reason`, `product_id` | USD | Lost revenue |
| `order_processing_time_seconds` | Histogram | `service_name`, `status`, `reason` | seconds | Processing time |
| `sla_violation_events` | Counter | `service_name`, `reason` | count | SLA violations |

---

## Conclusion

These improvements transform the OpenTelemetry demo from a basic instrumentation example into a comprehensive APM platform suitable for production-like observability scenarios.

### Key Achievements

‚úÖ **Complete RED metrics** for both services
‚úÖ **Business metrics** aligned with executive decision-making
‚úÖ **Automatic instrumentation** reduces developer burden
‚úÖ **Trace-log-metric correlation** enables root cause analysis
‚úÖ **Realistic traffic generation** for testing and demos
‚úÖ **Production-ready** patterns and best practices

### Next Steps

**For Production Deployment:**

1. **Enable AlertManager:**
   ```yaml
   # prometheus.yaml
   alertmanager:
     enabled: true
   ```

2. **Configure Grafana alerts** on critical metrics

3. **Implement tail-based sampling** for high-volume services

4. **Set up long-term storage** for metrics (e.g., Thanos, Cortex)

5. **Add anomaly detection** using Grafana ML or Prometheus-ML

6. **Implement SLO tracking** using Google SRE frameworks

**For Further Enhancement:**

- Add user journey tracking with session IDs
- Implement feature flag metrics
- Add canary deployment metrics
- Include database performance metrics
- Add cache hit/miss ratios
- Implement custom business KPIs

---

## References

- [OpenTelemetry Documentation](https://opentelemetry.io/docs/)
- [Prometheus Best Practices](https://prometheus.io/docs/practices/)
- [Grafana Dashboard Best Practices](https://grafana.com/docs/grafana/latest/dashboards/build-dashboards/best-practices/)
- [Site Reliability Engineering Book](https://sre.google/books/)
- [RED Method](https://www.weave.works/blog/the-red-method-key-metrics-for-microservices-architecture/)

---

**Document Version:** 1.0
**Last Updated:** 2026-01-13
**Authors:** Claude Code Engineering Team
